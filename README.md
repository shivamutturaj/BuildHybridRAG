# BuildHybridRAG
Hybrid RAG

rag_project/
â”œâ”€â”€ app.py                  # (your Streamlit app entry point)
â”œâ”€â”€ rag_pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py       # (for Confluence/Git data extraction)
â”‚   â”œâ”€â”€ embedding.py         # (embedding model)
â”‚   â”œâ”€â”€ vector_store.py      # (vector DBs: ChromaDB/FAISS)
â”‚   â”œâ”€â”€ llm_loader.py        # (open-source and OpenAI models)
â”‚   â”œâ”€â”€ retrieval.py         # (retriever logic)
â”‚   â”œâ”€â”€ prompt_template.py   # (dynamic prompt generator)
â”‚   â”œâ”€â”€ generator.py         # (generate final answer)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                # (optional: explain usage)
â””â”€â”€ config.py                # (optional: configs for models, db, etc.)

-----------------------------------------------------------------------------------
Data loader:
 -Crawl full Confluence spaces | âœ…
 -Crawl full GitHub repos | âœ…
 -Local folder support | âœ…
 -Full-source hyperlinks in answers | âœ…
 -Clean text parsing | âœ…

-----------------------------------------------------------------------------------
Problem:
Some Confluence pages or GitHub READMEs are very long (5K-10K tokens).
LLMs (like Mistral, LLaMA) have limited context windows (4Kâ€“32K tokens).

ðŸ‘‰ So we chunk long docs into smaller, manageable pieces!

-----------------------------------------------------------------------------------

Problem:
Even after retrieval, not all top_k results are perfectly relevant.

Solution:
 -After getting top 10 candidates, re-rank them by:
 -Cosine similarity
 -Embedding closeness to query

-----------------------------------------------------------------------------------

Extract Confluence + GitHub + Local
    â†“
Chunk large documents
    â†“
Embed (SentenceTransformer or HF)
    â†“
Store in ChromaDB / FAISS
    â†“
Retrieve (Top 10)
    â†“
Re-rank (Top 3 most similar)
    â†“
Prompt (LLM)
    â†“
Answer with Hyperlinks
--------------------------------------------------------------------------------------
ðŸŽ¯ Your RAG stack is now:

Feature	Status
Chunking large files	âœ…
Embedding optimized chunks	âœ…
Semantic re-ranking retrieval	âœ…
Metadata-based filtering	âœ…
Streamlit UI	âœ…
Source hyperlinks in answers	âœ…

-------------------------------------------------------------------------------------
CAG : 
Cache Embeddings to Disk (Avoid Recomputing)
âœ… Problem:
-Embedding millions of docs is slow.
-You don't want to recompute every restart!

âœ… Solution:
Save embeddings + metadata locally (e.g., .pkl or .parquet files).
------------------------------------------------------------------------------------
Hybrid Search (Semantic + Keyword)
âœ… Problem:
Semantic search alone can miss important keywords.
E.g., you ask: "Python GIL issues" â€” semantic match is OK, but "GIL" keyword is critical.

âœ… Solution:
Combine semantic similarity + keyword search.

-------------------------------------------------------------------------------------------
Expose as a FastAPI Server (RAG API Service)
âœ… Problem:
Streamlit is nice for UI but not for backend serving.
We need a real API to plug into apps, bots, workflows.

âœ… Solution:
Build a lightweight FastAPI app.

-------------------------------------------------------------------------------------------
Run api :
uvicorn app.rag_api:app --reload --host 0.0.0.0 --port 8000
-------------------------------------------------------------------------------------------
Example cURL Call:
curl -X POST "http://localhost:8000/ask" -H "Content-Type: application/json" -d '{"query":"What is the Python GIL?"}'
---------------------------------------------------------------------------------------------

[Confluence] + [GitHub] + [Local]
        â†“
 [Document Chunking + Cleaning]
        â†“
   [Embedding Cache Layer]
        â†“
 [ChromaDB / FAISS Vector DB]
        â†“
     [Hybrid Retriever]
        â†“
       [Prompt Templates]
        â†“
       [LLM Model]
        â†“
  [FastAPI Server for RAG API]

---------------------------------------------------------------------------------------------

our Complete RAG-as-a-Service has:

Data loaders (Confluence, GitHub, Local)	âœ…
Chunking long docs	âœ…
Embedding caching	âœ…
Hybrid retrieval (semantic + keyword)	âœ…
Hyperlinking sources	âœ…
Re-ranking documents	âœ…
Modular design (easy swap LLMs, DBs)	âœ…
Streamlit frontend (optional)	âœ…
FastAPI backend server (RAG API)	âœ…

--------------------------------------------------------------------------------------------------

Dockerize Everything (FastAPI + Vector DB)