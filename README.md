# BuildHybridRAG
Hybrid RAG

rag_project/
â”œâ”€â”€ app.py                  # (your Streamlit app entry point)
â”œâ”€â”€ rag_pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py       # (for Confluence/Git data extraction)
â”‚   â”œâ”€â”€ embedding.py         # (embedding model)
â”‚   â”œâ”€â”€ vector_store.py      # (vector DBs: ChromaDB/FAISS)
â”‚   â”œâ”€â”€ llm_loader.py        # (open-source and OpenAI models)
â”‚   â”œâ”€â”€ retrieval.py         # (retriever logic)
â”‚   â”œâ”€â”€ prompt_template.py   # (dynamic prompt generator)
â”‚   â”œâ”€â”€ generator.py         # (generate final answer)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md                # (optional: explain usage)
â””â”€â”€ config.py                # (optional: configs for models, db, etc.)

-----------------------------------------------------------------------------------
Data loader:
 -Crawl full Confluence spaces | âœ…
 -Crawl full GitHub repos | âœ…
 -Local folder support | âœ…
 -Full-source hyperlinks in answers | âœ…
 -Clean text parsing | âœ…

-----------------------------------------------------------------------------------
Problem:
Some Confluence pages or GitHub READMEs are very long (5K-10K tokens).
LLMs (like Mistral, LLaMA) have limited context windows (4Kâ€“32K tokens).

ðŸ‘‰ So we chunk long docs into smaller, manageable pieces!

-----------------------------------------------------------------------------------

Problem:
Even after retrieval, not all top_k results are perfectly relevant.

Solution:
 -After getting top 10 candidates, re-rank them by:
 -Cosine similarity
 -Embedding closeness to query

-----------------------------------------------------------------------------------

Extract Confluence + GitHub + Local
    â†“
Chunk large documents
    â†“
Embed (SentenceTransformer or HF)
    â†“
Store in ChromaDB / FAISS
    â†“
Retrieve (Top 10)
    â†“
Re-rank (Top 3 most similar)
    â†“
Prompt (LLM)
    â†“
Answer with Hyperlinks
--------------------------------------------------------------------------------------
ðŸŽ¯ Your RAG stack is now:

Feature	Status
Chunking large files	âœ…
Embedding optimized chunks	âœ…
Semantic re-ranking retrieval	âœ…
Metadata-based filtering	âœ…
Streamlit UI	âœ…
Source hyperlinks in answers	âœ…